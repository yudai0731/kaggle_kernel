{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Titanic : Tutorial\n### 参考記事「Kaggleに登録したら次にやること ～ これだけやれば十分闘える！Titanicの先へ行く入門 10 Kernel ～」\nLink : https://qiita.com/upura/items/3c10ff6fed4e7c3d70f0\n\nこのnotebookはversion2です.version1を見てない場合はversion1を先に見ましょう.versionの切り替えは https://www.kaggle.com/yudai0731/titanic-tutorial の右上青字のversionから切り替えることができます. version2ではversion1のnotebookを改良し,より精度の高いモデルを作成します."},{"metadata":{},"cell_type":"markdown","source":"## 1. まずはsubmit! 順位表に載ってみよう\nkaggleではいつくかの方法で自分が作成した機械学習モデルの予測結果を提出(submit)できる(Notebook経由のみのコンペもあり).\n* Notebook経由\n* csvファイルを直接アップロード\n* Kaggle APIを利用"},{"metadata":{},"cell_type":"markdown","source":"## 2 全体像を把握！ submitまでの処理の流れを見てみよう\n具体的な処理の流れ\n1. パッケージの読み込み\n2. データの読み込み\n3. 特徴量エンジニアリング(Feature Engineering)\n4. 機械学習アルゴリズムの学習・予測\n5. 提出"},{"metadata":{},"cell_type":"markdown","source":"### パッケージの読み込み\n以降の処理で利用するパッケージの読み込みを行う."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # road matrix calculation library \nimport pandas as pd # road data analysis library \nimport seaborn as sns # road visualization library \nimport matplotlib.pyplot as plt # road visualization library ","execution_count":14,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### データの読み込み\nkaggleから提供されているデータを読み込む.\nデータの詳細はkaggleのコンペティションのページの「Data」タブや,右上のDataの項目から見ることができる.  \ntrain.csvは機械学習の訓練用のデータです.test.csvは予測を実施するデータです.gender_submission.csvは提出のサンプルです.このファイルを見ると,提出ファイルの形式を確認できます.  \nhead関数を用いて実際にデータが読み込めていることを確認します.``train.head()``の実行結果を見ると,Pclass(搭乗しているクラス)やName(名前)など乗客一人一人の情報が表になっていることがわかります.\nこのコンペで予想するのはtestデータのSurvived(0:死亡,1:生存)という項目です.確認のために,``test.head()``の結果を見ると``Survived``の項目がないことがわかります.また``gender_submission.head()``\nの実行結果を見ると``PassengerId``と``Survived``が記述されています.  \n  \nデータをもう少し詳しくみると,``Name``や``Sex``などは文字列が格納されているため,そのままでは機械学習アルゴリズムの入力にすることができません.これらは機械学習アルゴリズムが扱える\n形に変換する必要があります.また,``Nan``というのはデータの欠損です.欠損値は,一部の機械学習アルゴリズムではそのまま扱うこともできますが,代表値で穴埋めする場合もあります."},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = '../input/titanic/' # set PATH string\ntrain = pd.read_csv(PATH+'train.csv') # read train data(csv)\ntest = pd.read_csv(PATH+'test.csv') # read train(csv)\ngender_submission = pd.read_csv(PATH+'gender_submission.csv') # read submit sample","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 実際に読み込めているか確認\ntrain.head() # trainの先頭5行を表示","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n3            4         1       1   \n4            5         0       3   \n\n                                                Name     Sex   Age  SibSp  \\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n2                             Heikkinen, Miss. Laina  female  26.0      0   \n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n4                           Allen, Mr. William Henry    male  35.0      0   \n\n   Parch            Ticket     Fare Cabin Embarked  \n0      0         A/5 21171   7.2500   NaN        S  \n1      0          PC 17599  71.2833   C85        C  \n2      0  STON/O2. 3101282   7.9250   NaN        S  \n3      0            113803  53.1000  C123        S  \n4      0            373450   8.0500   NaN        S  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Braund, Mr. Owen Harris</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>Heikkinen, Miss. Laina</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>53.1000</td>\n      <td>C123</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Allen, Mr. William Henry</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"   PassengerId  Pclass                                          Name     Sex  \\\n0          892       3                              Kelly, Mr. James    male   \n1          893       3              Wilkes, Mrs. James (Ellen Needs)  female   \n2          894       2                     Myles, Mr. Thomas Francis    male   \n3          895       3                              Wirz, Mr. Albert    male   \n4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n\n    Age  SibSp  Parch   Ticket     Fare Cabin Embarked  \n0  34.5      0      0   330911   7.8292   NaN        Q  \n1  47.0      1      0   363272   7.0000   NaN        S  \n2  62.0      0      0   240276   9.6875   NaN        Q  \n3  27.0      0      0   315154   8.6625   NaN        S  \n4  22.0      1      1  3101298  12.2875   NaN        S  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>892</td>\n      <td>3</td>\n      <td>Kelly, Mr. James</td>\n      <td>male</td>\n      <td>34.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>330911</td>\n      <td>7.8292</td>\n      <td>NaN</td>\n      <td>Q</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>893</td>\n      <td>3</td>\n      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n      <td>female</td>\n      <td>47.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>363272</td>\n      <td>7.0000</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>894</td>\n      <td>2</td>\n      <td>Myles, Mr. Thomas Francis</td>\n      <td>male</td>\n      <td>62.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>240276</td>\n      <td>9.6875</td>\n      <td>NaN</td>\n      <td>Q</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>895</td>\n      <td>3</td>\n      <td>Wirz, Mr. Albert</td>\n      <td>male</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>315154</td>\n      <td>8.6625</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>896</td>\n      <td>3</td>\n      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n      <td>female</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3101298</td>\n      <td>12.2875</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"gender_submission.head()","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"   PassengerId  Survived\n0          892         0\n1          893         1\n2          894         0\n3          895         0\n4          896         1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>892</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>893</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>894</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>895</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>896</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### 特徴量エンジニアリング\n次のような処理を特徴量エンジニアリングという.\n* 読み込んだデータを機械学習アルゴリズムが扱える形に変換する.\n* 既存のデータから,機械学習アルゴリズムが予測する上で有用な新しい特徴量を生成する.\n前者については,例えば``Sex``の``male``,``female``をそれぞれ0と1に変換します.また欠損値を埋める処理も行います.後者については次のNotebookで詳しく掘り下げていきます.  \n  \n大元のデータから特徴量エンジニアリングを経て,X_train,y_train,X_testというデータを作ります.大雑把な表現をすると,X_train,y_trainの対応関係を学習し,\nX_testに対応する(未知の)y_testの値を当てるという仕組みです.このような仕組みを「教師あり学習」と呼びます.(cf.教師なし学習)"},{"metadata":{},"cell_type":"markdown","source":"## 3. ここで差がつく！ 仮説に基づいて新しい特徴量を作ってみよう\n3~7章ではversion1のnotebookに手を加えながらスコアを上げる方法を学んでいきます.\n\n### 再現性の大切さ\n「再現性がある」とは,何度実行しても同じ結果が得られることです.kaggleでいうと同一スコアが得られることと言い換えられます.再現性がないと結果ごとに異なるスコアが得られてしまいます.\n今後,特徴量エンジニアリングなどでスコアの向上を試みても,予測モデルが改善されたか否かを正しく判断できなくなる問題が生じます.  \n  \n(欠損値を乱数で埋めてしまうと再現性がなくなってしまう例)\nversion1では欠損値があるためAgeを用いませんでしたが,年齢も乗客の生死を判断するうえで重要な特徴量です.ここでは欠損値を埋める方法を埋める方法を考えます.\n下のプログラムのように乱数でAgeの欠損値を埋めると再現性がなくなってしまいます.\n```python\nage_avg = data['Age'].mean() #mean of Age(mean:算術平均)\nage_std = data['Age'].std() # std of Age(std:標準偏差)\ndata['Age'].fillna(np.random.randint(age_avg - age_std, age_avg + age_std), inplace=True) #欠損値を平均から幅stdの範囲の整数乱数で埋める\n```\n\n再現性を確保する方法として,例えば次のような方法が考えられます.\n1. そもそも乱数を用いて部分を削除する.\n2. 乱数のseedを与えて実行結果を固定する.\n\nAgeについては乱数を用いるより欠損していないデータの中央値(median)を与えた方が筋の良い補完ができそうです.ここでは中央値で補完するコードを追加します."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.concat((train,test),ignore_index=True) # concat train and test\n# print(pd.isnull(dataset).sum()) # the number of missing values in each feature\ndataset['Age'] = dataset['Age'].fillna(dataset['Age'].median()) # fill missing value in Age(median)\ndataset.replace(['male','female'], [0, 1], inplace=True) # convert Sex into numbers\ndataset['Embarked'] = dataset['Embarked'].fillna('S') # fill missing value in Embarked(mode)\ndataset['Fare'] = dataset['Fare'].fillna(dataset['Fare'].mean())\ndataset.replace(['S','C','Q'], [0, 1,2], inplace=True) # convert Embarked into numbers\ndataset.head()","execution_count":37,"outputs":[{"output_type":"execute_result","execution_count":37,"data":{"text/plain":"   PassengerId  Survived  Pclass  \\\n0            1       0.0       3   \n1            2       1.0       1   \n2            3       1.0       3   \n3            4       1.0       1   \n4            5       0.0       3   \n\n                                                Name  Sex   Age  SibSp  Parch  \\\n0                            Braund, Mr. Owen Harris    0  22.0      1      0   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...    1  38.0      1      0   \n2                             Heikkinen, Miss. Laina    1  26.0      0      0   \n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)    1  35.0      1      0   \n4                           Allen, Mr. William Henry    0  35.0      0      0   \n\n             Ticket     Fare Cabin  Embarked  FamilySize  IsAlone  \n0         A/5 21171   7.2500   NaN         0         2.0      0.0  \n1          PC 17599  71.2833   C85         1         2.0      0.0  \n2  STON/O2. 3101282   7.9250   NaN         0         1.0      1.0  \n3            113803  53.1000  C123         0         2.0      0.0  \n4            373450   8.0500   NaN         0         1.0      1.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n      <th>FamilySize</th>\n      <th>IsAlone</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>Braund, Mr. Owen Harris</td>\n      <td>0</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n      <td>1</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>1</td>\n      <td>2.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1.0</td>\n      <td>3</td>\n      <td>Heikkinen, Miss. Laina</td>\n      <td>1</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n      <td>1</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>53.1000</td>\n      <td>C123</td>\n      <td>0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>Allen, Mr. William Henry</td>\n      <td>0</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.isnull(dataset).sum() # 欠損値がなくなったことを確認するコード 特徴量と欠損値の数が表示される","execution_count":38,"outputs":[{"output_type":"execute_result","execution_count":38,"data":{"text/plain":"PassengerId       0\nSurvived        418\nPclass            0\nName              0\nSex               0\nAge               0\nSibSp             0\nParch             0\nTicket            0\nFare              0\nCabin          1014\nEmbarked          0\nFamilySize      418\nIsAlone         418\ndtype: int64"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### 仮説から新しい特徴量を作る\nここでは,仮説と可視化から新しい特徴量を作る過程をまとめます.予測精度に寄与する新しい特徴量を製作るにあたっては,仮説と可視化を繰り返すサイクルが大切だと思います.\n* 予測精度に寄与しそうな仮説を立てる.\n* 可視化を実施する\n    - 予測精度に寄与する仮設を見つけるため\n    - 仮説が正しいか検証するため\n    \n### ケース1) ドメイン知識がある場合\n自分が詳しい,つまりドメイン知識を持っている分野の問題に取り組む場合,最初から仮説がいくつかあると思います.その場合は仮説を検証するような可視化を実施し,本当に予測精度に寄与するかを確認します.\n可視化の結果によっては,改めて仮説を立てることになるかもしれません.\n\n### ケース2) ドメイン知識がない場合\nドメイン知識がない場合は,まず仮説を立てるための探索的データ分析(EDA : Explore Data Analysis)を実施することになるでしょう.いろいろな軸でデータを眺め,予測精度に寄与しそうな仮説を立てるのが\n目的になります.  \n  \n実際に新しい特徴量を作ってみましょう.例として,EDAを実施した結果,ぼんやりと「一緒に乗船した家族の人数が多い方が,生存率が低そうだ」という仮説が得られた状況を考えます.\n仮説が得られたので次はこの仮説を検証するための可視化に移ります.version1で説明しませんでしたがそれぞれの特徴量の概要や意味についてはkaggleのコンペティションのページの「Data」タグに\n記述があります(英語).ざっくりですが特徴量の意味は次のとおりです.\n- PassengerID : 乗客の番号,予測には用いない\n- Survived : ターゲット,乗客の生死,生存1/死亡0\n- Pclass : 乗船していたクラス(1st,2nd,3rd)\n- Name : 乗客の名前\n- Sex : 乗客の性別(male,female)\n- Age : 乗客の年齢\n- SibSp : タイタニックに乗船していた兄弟,配偶者の人数\n- Parch : タイタニックに乗船していた両親,子供の人数\n- Ticket : チケット\n- Fare : 乗船運賃\n- Cabin : キャビン番号\n- Embarked : 乗船した港(C,Q,S)\n  \n今回の仮説では``SibSp``と``Parch``が関係しそうです.\n``FamilySize``という新しい行を作り,その大きさごとに生存したか否かを棒グラフにしました."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['FamilySize'] = dataset['Parch'] + dataset['SibSp'] + 1 # 'FamilySize = 兄弟,配偶者の塔乗数+両親,子供の塔乗数+自分(1)\ntrain['FamilySize'] = dataset['FamilySize'][:len(train)]\ntest['FamilySize'] = dataset['FamilySize'][len(train):]\nplt.figure(facecolor=\"white\")\nsns.countplot(x='FamilySize', data=train, hue='Survived')","execution_count":39,"outputs":[{"output_type":"execute_result","execution_count":39,"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7ff076fd7bd0>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"``FamilySize``が5以上の時,死亡が生存を上回っており,生存率が低いことがわかります.「一緒に乗船した家族の人数が多い方が,生存率が低そうだ」というぼんやりした仮説が,\n可視化を通じて「``FamilySize``が5以上の場合,生存率が低いので,この特徴量は予測精度に寄与しそうだ」という確信をもった仮説に変わりました.  \n  \n更に,今回の可視化を通じて「``FamilySize``が1の人が圧倒的に多く,生存率が低い」という新しい仮説を得ることもできました.\nこの``FamilySize``が1という特徴量も予測精度に寄与しそうなので,新しく``IsAlone``という特徴量を作成しました."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['IsAlone'] = 0\ndataset.loc[dataset['FamilySize']==1,'IsAlone'] = 1\ntrain['IsAlone'] = dataset['IsAlone'][:len(train)]\ntest['IsAlone'] = dataset['IsAlone'][len(train):]","execution_count":40,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"このように仮説と可視化を繰り返すことで,既存のデータから機械学習アルゴリズムが予測する上で有用な新しい特徴量を探索していきましょう.  \n  \n作成した特徴量が「有用」だったかを判断するには,例えば次の4パターンで学習した結果を提出する方法があります.提出した際のスコアを見ることで,特徴量の有用性をある程度確認可能です.\n- FamilySizeとIsAloneを加えた場合\n- FamilySizeのみを加えた場合\n- IsAloneのみを加えた場合\n- FamilySizeとIsAloneを加えていない場合\n\nここで「ある程度」としている理由については6章で学びます.\n\n(更に参考) \n- 書籍: 機械学習のための特徴量エンジニアリング(オライリー)\n- スライド : 最近のKaggleに学ぶテーブルデータの特徴量エンジニアリング https://www.slideshare.net/mlm_kansai/kaggle-138546659\n- ブログ :【随時更新】Kaggleテーブルデータコンペできっと役立つTipsまとめ https://naotaka1128.hatenadiary.jp/entry/kaggle-compe-tips"},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove unused features\ndrop_features = ['PassengerId','Name','Ticket','Cabin']\ndataset.drop(labels = drop_features, axis = 1, inplace = True)\n# make X_train\nX_train = dataset[:len(train)]\n# make y_train\ny_train = X_train['Survived']\nX_train = X_train.drop(labels = 'Survived',axis = 1)\n# make test\nX_test = dataset[len(train):]\nX_test = X_test.drop('Survived',axis=1)","execution_count":41,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train # できているか確認","execution_count":42,"outputs":[{"output_type":"execute_result","execution_count":42,"data":{"text/plain":"     Pclass  Sex   Age  SibSp  Parch     Fare  Embarked  FamilySize  IsAlone\n0         3    0  22.0      1      0   7.2500         0           2        0\n1         1    1  38.0      1      0  71.2833         1           2        0\n2         3    1  26.0      0      0   7.9250         0           1        1\n3         1    1  35.0      1      0  53.1000         0           2        0\n4         3    0  35.0      0      0   8.0500         0           1        1\n..      ...  ...   ...    ...    ...      ...       ...         ...      ...\n886       2    0  27.0      0      0  13.0000         0           1        1\n887       1    1  19.0      0      0  30.0000         0           1        1\n888       3    1  28.0      1      2  23.4500         0           4        0\n889       1    0  26.0      0      0  30.0000         1           1        1\n890       3    0  32.0      0      0   7.7500         2           1        1\n\n[891 rows x 9 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pclass</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n      <th>Embarked</th>\n      <th>FamilySize</th>\n      <th>IsAlone</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>0</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7.2500</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>71.2833</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.9250</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>53.1000</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>0</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.0500</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>886</th>\n      <td>2</td>\n      <td>0</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>13.0000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>887</th>\n      <td>1</td>\n      <td>1</td>\n      <td>19.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30.0000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>888</th>\n      <td>3</td>\n      <td>1</td>\n      <td>28.0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>23.4500</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>889</th>\n      <td>1</td>\n      <td>0</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30.0000</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>890</th>\n      <td>3</td>\n      <td>0</td>\n      <td>32.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.7500</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>891 rows × 9 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### 機械学習アルゴリズムの学習・予測\n用意した特徴量と予測の対象のペアから,機械学習アルゴリズムを用いて予測器を学習させましょう.\nまず,予測器の読み込みを行います.次に読み込んだ予測器をインスタンス化します.そして``fit``関数を用いて学習を行います.最後に予測値が未知の特徴量(X_test)を与えて予測させます(``predict``関数).\ny_predの中身は0と1の予測値が格納されています.  \n  \nversion1のコードを見直すと,LogisticRegressionのインスタンス化時に``random_state=0``という風に乱数seedを固定していました.\n\n```python \nfrom sklearn.linear_model import LogisticRegression \n\nclf = LogisticRegression(penalty='l2', solver='sag', random_state=0) # instantiate LogisticRegression\nclf.fit(X_train, y_train) # learning \n\ny_pred = clf.predict(X_test) # predict  \n```"},{"metadata":{},"cell_type":"markdown","source":"# 4. 勾配ブースティングが最強？！ いろいろな機械学習アルゴリズムを使ってみよう\nこれまでは機械学習アルゴリズムとしてロジスティック回帰(LogisticRegression)を採用していました.このnotebookではいろいろな機械学習アルゴリズムを採用してみましょう.  \n  \nロジスティック回帰の実装に利用したsklearn(scikit-learn:サイキットラーン)というパッケージは入出力インターフェースが統一されており,手軽に機械学習アルゴリズムを変更できます.  \n  \n最近のKaggleのコンペティション上位陣が利用している機械学習アルゴリズムとしては,勾配ブースティングやニューラルネットワークがあげられます.これらはロジスティック回帰に比べて\n表現力が高く,高性能に予測できる可能性を秘めています.特に上位陣で採用率が高いのは,「LightGBM」という勾配ブースティングのパッケージです."},{"metadata":{},"cell_type":"markdown","source":"### sklearn\nまずはsklearn内の機械学習アルゴリズムを変更してみましょう.clfで宣言するモデルを切り替えるだけで機械学習アルゴリズムを差し替えられます.例として,\nランダムフォレストと呼ばれる機械学習アルゴリズムを使ってみましょう.ランダムフォレストでの予測結果を提出すると0.77990というロジスティック回帰より格段に良いスコアがでました."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier \n\nclf = RandomForestClassifier(n_estimators=100,max_depth=2 ,random_state=0)\nclf.fit(X_train, y_train) # learning \n\ny_pred = clf.predict(X_test) # predict  \ny_pred[:10]","execution_count":27,"outputs":[{"output_type":"execute_result","execution_count":27,"data":{"text/plain":"array([0., 1., 0., 0., 1., 0., 1., 0., 1., 0.])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### LightGBM\n続いて,LightGBMを使います.sklearnとの差異もあり,いくつか下準備が必要です.\n1. 学習用・検証用にデータセットを分割する.\n2. カテゴリ変数をリスト形式で宣言する.\n\nLightBGMは大量の決定木を作成しながら学習を進めます.そのため,学習に利用したデータセットなどのみに過剰に適合し,本来の目的である未知の値に対する性能が劣化してしまう「過学習」という現象に陥りががちです.\nそこで学習に利用しない検証用のデータに対する性能を見ながら学習を打ち切る「early stopping」を利用するのが一般的となっています.\nここでは、X_trainをX_train（学習用）とX_valid（検証用）に分割します."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.3, random_state=0, stratify=y_train)","execution_count":28,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LightGBMでは,カテゴリ変数に対して特別な処理を自動的に実行してくれます.次のように,何をカテゴリ変数として扱ってほしいか明示的にLightGBMに教えてあげましょう."},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features = ['Embarked', 'Pclass', 'Sex']","execution_count":29,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"これで下準備が完了したので,LightGBMで学習・予測を実施します."},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\n\n\nlgb_train = lgb.Dataset(X_train, y_train, categorical_feature=categorical_features)\nlgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train, categorical_feature=categorical_features)\n\nparams = {\n    'objective': 'binary'\n}\n\nmodel = lgb.train(params, lgb_train,\n                  valid_sets=[lgb_train, lgb_eval],\n                  verbose_eval=10,\n                  num_boost_round=1000,\n                  early_stopping_rounds=10)\n\ny_pred = model.predict(X_test, num_iteration=model.best_iteration)","execution_count":30,"outputs":[{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.425241\tvalid_1's binary_logloss: 0.478975\n[20]\ttraining's binary_logloss: 0.344972\tvalid_1's binary_logloss: 0.444039\n[30]\ttraining's binary_logloss: 0.301357\tvalid_1's binary_logloss: 0.436304\n[40]\ttraining's binary_logloss: 0.265535\tvalid_1's binary_logloss: 0.438139\nEarly stopping, best iteration is:\n[38]\ttraining's binary_logloss: 0.271328\tvalid_1's binary_logloss: 0.435633\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred[:10] # 予測結果の先頭10個を表示","execution_count":31,"outputs":[{"output_type":"execute_result","execution_count":31,"data":{"text/plain":"array([0.0320592 , 0.34308916, 0.09903007, 0.05723199, 0.39919906,\n       0.22299318, 0.55036246, 0.0908458 , 0.78109016, 0.01881392])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"今回のLightGBMの設定では,出力結果は1になる予測値になります.そこで,しきい値を決め打って,0.5を上回っていれば1と予測したと見なして,提出してみます."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = (y_pred > 0.5).astype(int)\ny_pred[:10]","execution_count":32,"outputs":[{"output_type":"execute_result","execution_count":32,"data":{"text/plain":"array([0, 0, 0, 0, 0, 0, 1, 0, 1, 0])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"LightGBMでの予測結果を提出してみると,0.75598というスコアが出ました.ランダムフォレスト同様,ロジスティック回帰の時のスコアよりも向上しているのが分かります.\n\nこのように利用する機械学習アルゴリズム次第で、Kaggleのスコアを向上させることが可能です。\n勾配ブースティング系では今回紹介したLightGBM以外に,数年前から人気の根強い「XGBoost」や,主流とは言い難いですが徐々に頭角を現している「CatBoost」などがあります.\nまた「PyTorch」,「TensorFlow」などのパッケージを用いてニューラルネットワークを実装する場合もあります."},{"metadata":{},"cell_type":"markdown","source":"# 5. 機械学習アルゴリズムのお気持ち？！ ハイパーパラメータを調整してみよう\n機械学習アルゴリズムのふるまいはハイパーパラメータという値で制御されます.もちろん,ハイパーパラメータの値次第で予測結果は変わります.  \n  \nハイパーパラメータの調整は,主に2種類の方法があります.\n- 手動で調整する.\n- チューニングツールを使う.\n\n後者としては,Grid search,Bayesian Optimization,Hyperopt,Optunaなど,いくつかのツールがあります.このnotebookでは最初に手動でハイパーパラメータを調整し,\nその後,Optunaを用いたチューニングを実施します.  \n  \n最近のkaggleのコンペティションでは,データサイズが大きいため,上記のツールでのハイパーパラメータ調整が現実的な時間で終わらない問題もあります.また一般に,ハイパーパラメータでの\nスコアの上がり幅は特徴量エンジニアリングでよい特徴量を見つけた場合に劣るので,あまり時間をかけず手動で微調整する場合も少なくないように感じます."},{"metadata":{},"cell_type":"markdown","source":"### 手動で調整\nここでは,LightGBMの精度の向上を試みます.これまではobjectiveのみを指定していました.明示的に指定しない場合は``default``の値が自動的に定義されます.\n公式documentationの「Parameters Tuning」(Link : https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html)に従って,手動で調整を行っていきましょう.\nいくつかのユースケース別にハイパーパラメータ調整のTipsが掲載されています.今回は,精度を高めるのが目的なので「For Better Accuracy」を参照します.\n- Use large max_bin (may be slower) (大きめの``max_bin``を使え : defaultが255だからここでは300に設定)\n- Use small learning_rate with large num_iterations (小さめの``learning_rate``を使え : defaultの値は0.1だから,ここでは0.05に設定)\n- Use large num_leaves (may cause over-fitting) (大きめの``num_leaves``を使え : defaultの値は31だから,ここでは40に設定)\n- Use bigger training data\n- Try dart\n\n手動で調整するにせよ,チューニングツールを使うにせよ,機械学習アルゴリズムをブラックボックス的に利用するのではなく,ハイパーパラメータを正しく理解することが非常に大切です."},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'objective':'binary',\n    'max_bin':300,\n    'learning_rate':0.05,\n    'num_leaves':40\n}\n\nlgb_train = lgb.Dataset(X_train, y_train, categorical_feature=categorical_features)\nlgb_eval = lgb.Dataset(X_valid, y_valid,\n                       reference=lgb_train,\n                       categorical_feature=categorical_features)\n\nmodel = lgb.train(params, lgb_train, valid_sets=[lgb_train, lgb_eval],\n                  verbose_eval=10,\n                  num_boost_round=1000,\n                  early_stopping_rounds=10)\n\ny_pred = model.predict(X_test, num_iteration=model.best_iteration)","execution_count":33,"outputs":[{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.505699\tvalid_1's binary_logloss: 0.532106\n[20]\ttraining's binary_logloss: 0.427825\tvalid_1's binary_logloss: 0.482279\n[30]\ttraining's binary_logloss: 0.377242\tvalid_1's binary_logloss: 0.456641\n[40]\ttraining's binary_logloss: 0.345424\tvalid_1's binary_logloss: 0.447083\n[50]\ttraining's binary_logloss: 0.323113\tvalid_1's binary_logloss: 0.440407\n[60]\ttraining's binary_logloss: 0.302727\tvalid_1's binary_logloss: 0.434527\n[70]\ttraining's binary_logloss: 0.285597\tvalid_1's binary_logloss: 0.434932\nEarly stopping, best iteration is:\n[66]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.433251\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"y_predはハイパーパラメータ変更前と異なる値を取ります.出力ログにも変化があり,最終的なvalid_1's binary_loglossが 0.433251 と,変更前よりも小さい値になっています.binary_loglossは損失なので,小さい方が望ましいです.  \n  \nLightGBMでの予測結果を提出してみると,0.77033というスコアが出ました.ハイパーパラメータ変更前の0.75598に比べて,スコアが向上しています."},{"metadata":{},"cell_type":"markdown","source":"### Optunaを使う\nここまで手動でハイパーパラメータを調整してきましたが,次のような感情が芽生えた方もいるのではないでしょうか.\n- 「大きめ」「小さめ」といっても,具体的にどの値にすればよいかわからない.\n- 各パラメータの組み合わせ方もいくつかあり,逐一設定・実行して性能を検証するのは煩わしい\n\nそのような課題を解決してくれるのが,ハイパーパラメータのチューニングツールです.今回はOptunaを使っていきます.\nOptunaを使うに当たっては,あらかじめ次の関数の``trial.suggest_int()``のように,探索範囲を定義します.  \n  \nここでは,意図的にlearning_rateの調整を実施していません.テーブルデータをLightGBMで扱う場合,一般にlearning_rateが低いほど高い性能が得られるためです.そのため探索範囲には含めず,必要であれば後に手動で低い値に変更することにします."},{"metadata":{"trusted":true},"cell_type":"code","source":"import optuna\nfrom sklearn.metrics import log_loss\n\n\ndef objective(trial):\n    params = {\n        'objective': 'binary',\n        'max_bin': trial.suggest_int('max_bin', 255, 500),\n        'learning_rate': 0.05,\n        'num_leaves': trial.suggest_int('num_leaves', 32, 128),\n    }\n\n    lgb_train = lgb.Dataset(X_train, y_train,\n                            categorical_feature=categorical_features)\n    lgb_eval = lgb.Dataset(X_valid, y_valid,\n                           reference=lgb_train,\n                           categorical_feature=categorical_features)\n\n    model = lgb.train(params, lgb_train,\n                      valid_sets=[lgb_train, lgb_eval],\n                      verbose_eval=10,\n                      num_boost_round=1000,\n                      early_stopping_rounds=10)\n\n    y_pred_valid = model.predict(X_valid, num_iteration=model.best_iteration)\n    score = log_loss(y_valid, y_pred_valid)\n    return score","execution_count":34,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"``n_trials``は試行回数です.ここでは計算を短くするため,40回程度にしておきます.乱数も固定しておきます."},{"metadata":{"trusted":true},"cell_type":"code","source":"study = optuna.create_study(sampler=optuna.samplers.RandomSampler(seed=0))\nstudy.optimize(objective, n_trials=40)\nstudy.best_params","execution_count":35,"outputs":[{"output_type":"stream","text":"[I 2020-10-02 12:33:18,799] A new study created in memory with name: no-name-2b1d3bae-2037-440a-9733-06b5f9badf7c\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.505699\tvalid_1's binary_logloss: 0.532106\n[20]\ttraining's binary_logloss: 0.427825\tvalid_1's binary_logloss: 0.482279\n[30]\ttraining's binary_logloss: 0.377242\tvalid_1's binary_logloss: 0.456641\n[40]\ttraining's binary_logloss: 0.345424\tvalid_1's binary_logloss: 0.447083\n[50]\ttraining's binary_logloss: 0.323113\tvalid_1's binary_logloss: 0.440407\n[60]\ttraining's binary_logloss: 0.302727\tvalid_1's binary_logloss: 0.434527\n[70]\ttraining's binary_logloss: 0.285597\tvalid_1's binary_logloss: 0.434932\nEarly stopping, best iteration is:\n[66]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.433251\n","name":"stdout"},{"output_type":"stream","text":"[I 2020-10-02 12:33:18,995] Trial 0 finished with value: 0.4332512137886331 and parameters: {'max_bin': 427, 'num_leaves': 79}. Best is trial 0 with value: 0.4332512137886331.\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.505699\tvalid_1's binary_logloss: 0.532106\n[20]\ttraining's binary_logloss: 0.427825\tvalid_1's binary_logloss: 0.482279\n[30]\ttraining's binary_logloss: 0.377242\tvalid_1's binary_logloss: 0.456641\n[40]\ttraining's binary_logloss: 0.345424\tvalid_1's binary_logloss: 0.447083\n[50]\ttraining's binary_logloss: 0.323113\tvalid_1's binary_logloss: 0.440407\n[60]\ttraining's binary_logloss: 0.302727\tvalid_1's binary_logloss: 0.434527\n[70]\ttraining's binary_logloss: 0.285597\tvalid_1's binary_logloss: 0.434932\nEarly stopping, best iteration is:\n[66]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.433251\n","name":"stdout"},{"output_type":"stream","text":"[I 2020-10-02 12:33:19,155] Trial 1 finished with value: 0.4332512137886331 and parameters: {'max_bin': 372, 'num_leaves': 96}. Best is trial 0 with value: 0.4332512137886331.\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.505699\tvalid_1's binary_logloss: 0.532106\n[20]\ttraining's binary_logloss: 0.427825\tvalid_1's binary_logloss: 0.482279\n[30]\ttraining's binary_logloss: 0.377242\tvalid_1's binary_logloss: 0.456641\n[40]\ttraining's binary_logloss: 0.345424\tvalid_1's binary_logloss: 0.447083\n[50]\ttraining's binary_logloss: 0.323113\tvalid_1's binary_logloss: 0.440407\n[60]\ttraining's binary_logloss: 0.302727\tvalid_1's binary_logloss: 0.434527\n[70]\ttraining's binary_logloss: 0.285597\tvalid_1's binary_logloss: 0.434932\nEarly stopping, best iteration is:\n[66]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.433251\n","name":"stdout"},{"output_type":"stream","text":"[I 2020-10-02 12:33:19,326] Trial 2 finished with value: 0.4332512137886331 and parameters: {'max_bin': 322, 'num_leaves': 99}. Best is trial 0 with value: 0.4332512137886331.\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.505699\tvalid_1's binary_logloss: 0.532106\n[20]\ttraining's binary_logloss: 0.427825\tvalid_1's binary_logloss: 0.482279\n[30]\ttraining's binary_logloss: 0.377242\tvalid_1's binary_logloss: 0.456641\n[40]\ttraining's binary_logloss: 0.345424\tvalid_1's binary_logloss: 0.447083\n[50]\ttraining's binary_logloss: 0.323113\tvalid_1's binary_logloss: 0.440407\n[60]\ttraining's binary_logloss: 0.302727\tvalid_1's binary_logloss: 0.434527\n[70]\ttraining's binary_logloss: 0.285597\tvalid_1's binary_logloss: 0.434932\nEarly stopping, best iteration is:\n[66]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.433251\n","name":"stdout"},{"output_type":"stream","text":"[I 2020-10-02 12:33:19,485] Trial 3 finished with value: 0.4332512137886331 and parameters: {'max_bin': 358, 'num_leaves': 41}. Best is trial 0 with value: 0.4332512137886331.\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.505699\tvalid_1's binary_logloss: 0.532106\n[20]\ttraining's binary_logloss: 0.427825\tvalid_1's binary_logloss: 0.482279\n[30]\ttraining's binary_logloss: 0.377242\tvalid_1's binary_logloss: 0.456641\n[40]\ttraining's binary_logloss: 0.345424\tvalid_1's binary_logloss: 0.447083\n[50]\ttraining's binary_logloss: 0.323113\tvalid_1's binary_logloss: 0.440407\n[60]\ttraining's binary_logloss: 0.302727\tvalid_1's binary_logloss: 0.434527\n[70]\ttraining's binary_logloss: 0.285597\tvalid_1's binary_logloss: 0.434932\nEarly stopping, best iteration is:\n[66]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.433251\n","name":"stdout"},{"output_type":"stream","text":"[I 2020-10-02 12:33:19,640] Trial 4 finished with value: 0.4332512137886331 and parameters: {'max_bin': 466, 'num_leaves': 53}. Best is trial 0 with value: 0.4332512137886331.\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.505699\tvalid_1's binary_logloss: 0.532106\n[20]\ttraining's binary_logloss: 0.427825\tvalid_1's binary_logloss: 0.482279\n[30]\ttraining's binary_logloss: 0.377242\tvalid_1's binary_logloss: 0.456641\n[40]\ttraining's binary_logloss: 0.345424\tvalid_1's binary_logloss: 0.447083\n[50]\ttraining's binary_logloss: 0.323113\tvalid_1's binary_logloss: 0.440407\n[60]\ttraining's binary_logloss: 0.302727\tvalid_1's binary_logloss: 0.434527\n[70]\ttraining's binary_logloss: 0.285597\tvalid_1's binary_logloss: 0.434932\nEarly stopping, best iteration is:\n[66]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.433251\n","name":"stdout"},{"output_type":"stream","text":"[I 2020-10-02 12:33:19,819] Trial 5 finished with value: 0.4332512137886331 and parameters: {'max_bin': 497, 'num_leaves': 68}. Best is trial 0 with value: 0.4332512137886331.\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.505699\tvalid_1's binary_logloss: 0.532106\n[20]\ttraining's binary_logloss: 0.427825\tvalid_1's binary_logloss: 0.482279\n[30]\ttraining's binary_logloss: 0.377242\tvalid_1's binary_logloss: 0.456641\n[40]\ttraining's binary_logloss: 0.345424\tvalid_1's binary_logloss: 0.447083\n[50]\ttraining's binary_logloss: 0.323113\tvalid_1's binary_logloss: 0.440407\n[60]\ttraining's binary_logloss: 0.302727\tvalid_1's binary_logloss: 0.434527\n[70]\ttraining's binary_logloss: 0.285597\tvalid_1's binary_logloss: 0.434932\nEarly stopping, best iteration is:\n[66]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.433251\n","name":"stdout"},{"output_type":"stream","text":"[I 2020-10-02 12:33:20,002] Trial 6 finished with value: 0.4332512137886331 and parameters: {'max_bin': 342, 'num_leaves': 102}. Best is trial 0 with value: 0.4332512137886331.\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.505699\tvalid_1's binary_logloss: 0.532106\n[20]\ttraining's binary_logloss: 0.427825\tvalid_1's binary_logloss: 0.482279\n[30]\ttraining's binary_logloss: 0.377242\tvalid_1's binary_logloss: 0.456641\n[40]\ttraining's binary_logloss: 0.345424\tvalid_1's binary_logloss: 0.447083\n[50]\ttraining's binary_logloss: 0.323113\tvalid_1's binary_logloss: 0.440407\n[60]\ttraining's binary_logloss: 0.302727\tvalid_1's binary_logloss: 0.434527\n[70]\ttraining's binary_logloss: 0.285597\tvalid_1's binary_logloss: 0.434932\nEarly stopping, best iteration is:\n[66]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.433251\n","name":"stdout"},{"output_type":"stream","text":"[I 2020-10-02 12:33:20,160] Trial 7 finished with value: 0.4332512137886331 and parameters: {'max_bin': 471, 'num_leaves': 120}. Best is trial 0 with value: 0.4332512137886331.\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.505699\tvalid_1's binary_logloss: 0.532106\n[20]\ttraining's binary_logloss: 0.427825\tvalid_1's binary_logloss: 0.482279\n[30]\ttraining's binary_logloss: 0.377242\tvalid_1's binary_logloss: 0.456641\n[40]\ttraining's binary_logloss: 0.345424\tvalid_1's binary_logloss: 0.447083\n[50]\ttraining's binary_logloss: 0.323113\tvalid_1's binary_logloss: 0.440407\n[60]\ttraining's binary_logloss: 0.302727\tvalid_1's binary_logloss: 0.434527\n[70]\ttraining's binary_logloss: 0.285597\tvalid_1's binary_logloss: 0.434932\nEarly stopping, best iteration is:\n[66]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.433251\n","name":"stdout"},{"output_type":"stream","text":"[I 2020-10-02 12:33:20,320] Trial 8 finished with value: 0.4332512137886331 and parameters: {'max_bin': 395, 'num_leaves': 90}. Best is trial 0 with value: 0.4332512137886331.\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.505699\tvalid_1's binary_logloss: 0.532106\n[20]\ttraining's binary_logloss: 0.427825\tvalid_1's binary_logloss: 0.482279\n[30]\ttraining's binary_logloss: 0.377242\tvalid_1's binary_logloss: 0.456641\n[40]\ttraining's binary_logloss: 0.345424\tvalid_1's binary_logloss: 0.447083\n[50]\ttraining's binary_logloss: 0.323113\tvalid_1's binary_logloss: 0.440407\n[60]\ttraining's binary_logloss: 0.302727\tvalid_1's binary_logloss: 0.434527\n[70]\ttraining's binary_logloss: 0.285597\tvalid_1's binary_logloss: 0.434932\nEarly stopping, best iteration is:\n[66]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.433251\n","name":"stdout"},{"output_type":"stream","text":"[I 2020-10-02 12:33:20,484] Trial 9 finished with value: 0.4332512137886331 and parameters: {'max_bin': 448, 'num_leaves': 71}. Best is trial 0 with value: 0.4332512137886331.\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.505699\tvalid_1's binary_logloss: 0.532106\n[20]\ttraining's binary_logloss: 0.427825\tvalid_1's binary_logloss: 0.482279\n[30]\ttraining's binary_logloss: 0.377242\tvalid_1's binary_logloss: 0.456641\n[40]\ttraining's binary_logloss: 0.345424\tvalid_1's binary_logloss: 0.447083\n[50]\ttraining's binary_logloss: 0.323113\tvalid_1's binary_logloss: 0.440407\n[60]\ttraining's binary_logloss: 0.302727\tvalid_1's binary_logloss: 0.434527\n[70]\ttraining's binary_logloss: 0.285597\tvalid_1's binary_logloss: 0.434932\nEarly stopping, best iteration is:\n[66]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.433251\n","name":"stdout"},{"output_type":"stream","text":"[I 2020-10-02 12:33:20,662] Trial 10 finished with value: 0.4332512137886331 and parameters: {'max_bin': 342, 'num_leaves': 78}. Best is trial 0 with value: 0.4332512137886331.\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.505699\tvalid_1's binary_logloss: 0.532106\n[20]\ttraining's binary_logloss: 0.427825\tvalid_1's binary_logloss: 0.482279\n[30]\ttraining's binary_logloss: 0.377242\tvalid_1's binary_logloss: 0.456641\n[40]\ttraining's binary_logloss: 0.345424\tvalid_1's binary_logloss: 0.447083\n[50]\ttraining's binary_logloss: 0.323113\tvalid_1's binary_logloss: 0.440407\n[60]\ttraining's binary_logloss: 0.302727\tvalid_1's binary_logloss: 0.434527\n[70]\ttraining's binary_logloss: 0.285597\tvalid_1's binary_logloss: 0.434932\nEarly stopping, best iteration is:\n[66]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.433251\n","name":"stdout"},{"output_type":"stream","text":"[I 2020-10-02 12:33:20,827] Trial 11 finished with value: 0.4332512137886331 and parameters: {'max_bin': 343, 'num_leaves': 113}. Best is trial 0 with value: 0.4332512137886331.\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.505699\tvalid_1's binary_logloss: 0.532106\n[20]\ttraining's binary_logloss: 0.427825\tvalid_1's binary_logloss: 0.482279\n[30]\ttraining's binary_logloss: 0.377242\tvalid_1's binary_logloss: 0.456641\n[40]\ttraining's binary_logloss: 0.345424\tvalid_1's binary_logloss: 0.447083\n[50]\ttraining's binary_logloss: 0.323113\tvalid_1's binary_logloss: 0.440407\n[60]\ttraining's binary_logloss: 0.302727\tvalid_1's binary_logloss: 0.434527\n[70]\ttraining's binary_logloss: 0.285597\tvalid_1's binary_logloss: 0.434932\nEarly stopping, best iteration is:\n[66]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.433251\n","name":"stdout"},{"output_type":"stream","text":"[I 2020-10-02 12:33:20,993] Trial 12 finished with value: 0.4332512137886331 and parameters: {'max_bin': 420, 'num_leaves': 57}. Best is trial 0 with value: 0.4332512137886331.\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.505699\tvalid_1's binary_logloss: 0.532106\n[20]\ttraining's binary_logloss: 0.427825\tvalid_1's binary_logloss: 0.482279\n[30]\ttraining's binary_logloss: 0.377242\tvalid_1's binary_logloss: 0.456641\n[40]\ttraining's binary_logloss: 0.345424\tvalid_1's binary_logloss: 0.447083\n[50]\ttraining's binary_logloss: 0.323113\tvalid_1's binary_logloss: 0.440407\n[60]\ttraining's binary_logloss: 0.302727\tvalid_1's binary_logloss: 0.434527\n[70]\ttraining's binary_logloss: 0.285597\tvalid_1's binary_logloss: 0.434932\nEarly stopping, best iteration is:\n[66]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.433251\n","name":"stdout"},{"output_type":"stream","text":"[I 2020-10-02 12:33:21,161] Trial 13 finished with value: 0.4332512137886331 and parameters: {'max_bin': 332, 'num_leaves': 104}. Best is trial 0 with value: 0.4332512137886331.\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.505699\tvalid_1's binary_logloss: 0.532106\n[20]\ttraining's binary_logloss: 0.427825\tvalid_1's binary_logloss: 0.482279\n[30]\ttraining's binary_logloss: 0.377242\tvalid_1's binary_logloss: 0.456641\n[40]\ttraining's binary_logloss: 0.345424\tvalid_1's binary_logloss: 0.447083\n[50]\ttraining's binary_logloss: 0.323113\tvalid_1's binary_logloss: 0.440407\n[60]\ttraining's binary_logloss: 0.302727\tvalid_1's binary_logloss: 0.434527\n[70]\ttraining's binary_logloss: 0.285597\tvalid_1's binary_logloss: 0.434932\nEarly stopping, best iteration is:\n[66]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.433251\n","name":"stdout"},{"output_type":"stream","text":"[I 2020-10-02 12:33:21,361] Trial 14 finished with value: 0.4332512137886331 and parameters: {'max_bin': 264, 'num_leaves': 52}. Best is trial 0 with value: 0.4332512137886331.\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.505699\tvalid_1's binary_logloss: 0.532106\n[20]\ttraining's binary_logloss: 0.427825\tvalid_1's binary_logloss: 0.482279\n[30]\ttraining's binary_logloss: 0.377242\tvalid_1's binary_logloss: 0.456641\n[40]\ttraining's binary_logloss: 0.345424\tvalid_1's binary_logloss: 0.447083\n[50]\ttraining's binary_logloss: 0.323113\tvalid_1's binary_logloss: 0.440407\n[60]\ttraining's binary_logloss: 0.302727\tvalid_1's binary_logloss: 0.434527\n[70]\ttraining's binary_logloss: 0.285597\tvalid_1's binary_logloss: 0.434932\nEarly stopping, best iteration is:\n[66]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.433251\n","name":"stdout"},{"output_type":"stream","text":"[I 2020-10-02 12:33:21,532] Trial 15 finished with value: 0.4332512137886331 and parameters: {'max_bin': 370, 'num_leaves': 112}. Best is trial 0 with value: 0.4332512137886331.\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.505699\tvalid_1's binary_logloss: 0.532106\n[20]\ttraining's binary_logloss: 0.427825\tvalid_1's binary_logloss: 0.482279\n[30]\ttraining's binary_logloss: 0.377242\tvalid_1's binary_logloss: 0.456641\n[40]\ttraining's binary_logloss: 0.345424\tvalid_1's binary_logloss: 0.447083\n[50]\ttraining's binary_logloss: 0.323113\tvalid_1's binary_logloss: 0.440407\n[60]\ttraining's binary_logloss: 0.302727\tvalid_1's binary_logloss: 0.434527\n[70]\ttraining's binary_logloss: 0.285597\tvalid_1's binary_logloss: 0.434932\nEarly stopping, best iteration is:\n[66]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.433251\n","name":"stdout"},{"output_type":"stream","text":"[I 2020-10-02 12:33:21,697] Trial 16 finished with value: 0.4332512137886331 and parameters: {'max_bin': 498, 'num_leaves': 101}. Best is trial 0 with value: 0.4332512137886331.\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.505699\tvalid_1's binary_logloss: 0.532106\n[20]\ttraining's binary_logloss: 0.427825\tvalid_1's binary_logloss: 0.482279\n[30]\ttraining's binary_logloss: 0.377242\tvalid_1's binary_logloss: 0.456641\n[40]\ttraining's binary_logloss: 0.345424\tvalid_1's binary_logloss: 0.447083\n[50]\ttraining's binary_logloss: 0.323113\tvalid_1's binary_logloss: 0.440407\n[60]\ttraining's binary_logloss: 0.302727\tvalid_1's binary_logloss: 0.434527\n[70]\ttraining's binary_logloss: 0.285597\tvalid_1's binary_logloss: 0.434932\nEarly stopping, best iteration is:\n[66]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.433251\n","name":"stdout"},{"output_type":"stream","text":"[I 2020-10-02 12:33:21,864] Trial 17 finished with value: 0.4332512137886331 and parameters: {'max_bin': 334, 'num_leaves': 79}. Best is trial 0 with value: 0.4332512137886331.\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.505699\tvalid_1's binary_logloss: 0.532106\n[20]\ttraining's binary_logloss: 0.427825\tvalid_1's binary_logloss: 0.482279\n[30]\ttraining's binary_logloss: 0.377242\tvalid_1's binary_logloss: 0.456641\n[40]\ttraining's binary_logloss: 0.345424\tvalid_1's binary_logloss: 0.447083\n[50]\ttraining's binary_logloss: 0.323113\tvalid_1's binary_logloss: 0.440407\n[60]\ttraining's binary_logloss: 0.302727\tvalid_1's binary_logloss: 0.434527\n[70]\ttraining's binary_logloss: 0.285597\tvalid_1's binary_logloss: 0.434932\nEarly stopping, best iteration is:\n[66]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.433251\n","name":"stdout"},{"output_type":"stream","text":"[I 2020-10-02 12:33:22,043] Trial 18 finished with value: 0.4332512137886331 and parameters: {'max_bin': 447, 'num_leaves': 114}. Best is trial 0 with value: 0.4332512137886331.\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.505699\tvalid_1's binary_logloss: 0.532106\n[20]\ttraining's binary_logloss: 0.427825\tvalid_1's binary_logloss: 0.482279\n[30]\ttraining's binary_logloss: 0.377242\tvalid_1's binary_logloss: 0.456641\n[40]\ttraining's binary_logloss: 0.345424\tvalid_1's binary_logloss: 0.447083\n[50]\ttraining's binary_logloss: 0.323113\tvalid_1's binary_logloss: 0.440407\n[60]\ttraining's binary_logloss: 0.302727\tvalid_1's binary_logloss: 0.434527\n[70]\ttraining's binary_logloss: 0.285597\tvalid_1's binary_logloss: 0.434932\nEarly stopping, best iteration is:\n[66]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.433251\n","name":"stdout"},{"output_type":"stream","text":"[I 2020-10-02 12:33:22,219] Trial 19 finished with value: 0.4332512137886331 and parameters: {'max_bin': 354, 'num_leaves': 120}. Best is trial 0 with value: 0.4332512137886331.\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.505699\tvalid_1's binary_logloss: 0.532106\n[20]\ttraining's binary_logloss: 0.427825\tvalid_1's binary_logloss: 0.482279\n[30]\ttraining's binary_logloss: 0.377242\tvalid_1's binary_logloss: 0.456641\n[40]\ttraining's binary_logloss: 0.345424\tvalid_1's binary_logloss: 0.447083\n[50]\ttraining's binary_logloss: 0.323113\tvalid_1's binary_logloss: 0.440407\n[60]\ttraining's binary_logloss: 0.302727\tvalid_1's binary_logloss: 0.434527\n[70]\ttraining's binary_logloss: 0.285597\tvalid_1's binary_logloss: 0.434932\nEarly stopping, best iteration is:\n[66]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.433251\n","name":"stdout"},{"output_type":"stream","text":"[I 2020-10-02 12:33:22,398] Trial 20 finished with value: 0.4332512137886331 and parameters: {'max_bin': 432, 'num_leaves': 61}. Best is trial 0 with value: 0.4332512137886331.\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.505699\tvalid_1's binary_logloss: 0.532106\n[20]\ttraining's binary_logloss: 0.427825\tvalid_1's binary_logloss: 0.482279\n[30]\ttraining's binary_logloss: 0.377242\tvalid_1's binary_logloss: 0.456641\n[40]\ttraining's binary_logloss: 0.345424\tvalid_1's binary_logloss: 0.447083\n[50]\ttraining's binary_logloss: 0.323113\tvalid_1's binary_logloss: 0.440407\n[60]\ttraining's binary_logloss: 0.302727\tvalid_1's binary_logloss: 0.434527\n[70]\ttraining's binary_logloss: 0.285597\tvalid_1's binary_logloss: 0.434932\nEarly stopping, best iteration is:\n[66]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.433251\n","name":"stdout"},{"output_type":"stream","text":"[I 2020-10-02 12:33:22,564] Trial 21 finished with value: 0.4332512137886331 and parameters: {'max_bin': 402, 'num_leaves': 51}. Best is trial 0 with value: 0.4332512137886331.\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.505699\tvalid_1's binary_logloss: 0.532106\n[20]\ttraining's binary_logloss: 0.427825\tvalid_1's binary_logloss: 0.482279\n[30]\ttraining's binary_logloss: 0.377242\tvalid_1's binary_logloss: 0.456641\n[40]\ttraining's binary_logloss: 0.345424\tvalid_1's binary_logloss: 0.447083\n[50]\ttraining's binary_logloss: 0.323113\tvalid_1's binary_logloss: 0.440407\n[60]\ttraining's binary_logloss: 0.302727\tvalid_1's binary_logloss: 0.434527\n[70]\ttraining's binary_logloss: 0.285597\tvalid_1's binary_logloss: 0.434932\nEarly stopping, best iteration is:\n[66]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.433251\n","name":"stdout"},{"output_type":"stream","text":"[I 2020-10-02 12:33:22,750] Trial 22 finished with value: 0.4332512137886331 and parameters: {'max_bin': 397, 'num_leaves': 71}. Best is trial 0 with value: 0.4332512137886331.\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.505699\tvalid_1's binary_logloss: 0.532106\n[20]\ttraining's binary_logloss: 0.427825\tvalid_1's binary_logloss: 0.482279\n[30]\ttraining's binary_logloss: 0.377242\tvalid_1's binary_logloss: 0.456641\n[40]\ttraining's binary_logloss: 0.345424\tvalid_1's binary_logloss: 0.447083\n[50]\ttraining's binary_logloss: 0.323113\tvalid_1's binary_logloss: 0.440407\n[60]\ttraining's binary_logloss: 0.302727\tvalid_1's binary_logloss: 0.434527\n[70]\ttraining's binary_logloss: 0.285597\tvalid_1's binary_logloss: 0.434932\nEarly stopping, best iteration is:\n[66]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.433251\n","name":"stdout"},{"output_type":"stream","text":"[I 2020-10-02 12:33:22,914] Trial 23 finished with value: 0.4332512137886331 and parameters: {'max_bin': 287, 'num_leaves': 97}. Best is trial 0 with value: 0.4332512137886331.\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.505699\tvalid_1's binary_logloss: 0.532106\n[20]\ttraining's binary_logloss: 0.427825\tvalid_1's binary_logloss: 0.482279\n[30]\ttraining's binary_logloss: 0.377242\tvalid_1's binary_logloss: 0.456641\n[40]\ttraining's binary_logloss: 0.345424\tvalid_1's binary_logloss: 0.447083\n[50]\ttraining's binary_logloss: 0.323113\tvalid_1's binary_logloss: 0.440407\n[60]\ttraining's binary_logloss: 0.302727\tvalid_1's binary_logloss: 0.434527\n[70]\ttraining's binary_logloss: 0.285597\tvalid_1's binary_logloss: 0.434932\nEarly stopping, best iteration is:\n[66]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.433251\n","name":"stdout"},{"output_type":"stream","text":"[I 2020-10-02 12:33:23,068] Trial 24 finished with value: 0.4332512137886331 and parameters: {'max_bin': 264, 'num_leaves': 89}. Best is trial 0 with value: 0.4332512137886331.\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.505699\tvalid_1's binary_logloss: 0.532106\n[20]\ttraining's binary_logloss: 0.427825\tvalid_1's binary_logloss: 0.482279\n[30]\ttraining's binary_logloss: 0.377242\tvalid_1's binary_logloss: 0.456641\n[40]\ttraining's binary_logloss: 0.345424\tvalid_1's binary_logloss: 0.447083\n[50]\ttraining's binary_logloss: 0.323113\tvalid_1's binary_logloss: 0.440407\n[60]\ttraining's binary_logloss: 0.302727\tvalid_1's binary_logloss: 0.434527\n[70]\ttraining's binary_logloss: 0.285597\tvalid_1's binary_logloss: 0.434932\nEarly stopping, best iteration is:\n[66]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.433251\n","name":"stdout"},{"output_type":"stream","text":"[I 2020-10-02 12:33:23,220] Trial 25 finished with value: 0.4332512137886331 and parameters: {'max_bin': 382, 'num_leaves': 64}. Best is trial 0 with value: 0.4332512137886331.\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.505699\tvalid_1's binary_logloss: 0.532106\n[20]\ttraining's binary_logloss: 0.427825\tvalid_1's binary_logloss: 0.482279\n[30]\ttraining's binary_logloss: 0.377242\tvalid_1's binary_logloss: 0.456641\n[40]\ttraining's binary_logloss: 0.345424\tvalid_1's binary_logloss: 0.447083\n[50]\ttraining's binary_logloss: 0.323113\tvalid_1's binary_logloss: 0.440407\n[60]\ttraining's binary_logloss: 0.302727\tvalid_1's binary_logloss: 0.434527\n[70]\ttraining's binary_logloss: 0.285597\tvalid_1's binary_logloss: 0.434932\nEarly stopping, best iteration is:\n[66]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.433251\n","name":"stdout"},{"output_type":"stream","text":"[I 2020-10-02 12:33:23,374] Trial 26 finished with value: 0.4332512137886331 and parameters: {'max_bin': 286, 'num_leaves': 106}. Best is trial 0 with value: 0.4332512137886331.\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.505699\tvalid_1's binary_logloss: 0.532106\n[20]\ttraining's binary_logloss: 0.427825\tvalid_1's binary_logloss: 0.482279\n[30]\ttraining's binary_logloss: 0.377242\tvalid_1's binary_logloss: 0.456641\n[40]\ttraining's binary_logloss: 0.345424\tvalid_1's binary_logloss: 0.447083\n[50]\ttraining's binary_logloss: 0.323113\tvalid_1's binary_logloss: 0.440407\n[60]\ttraining's binary_logloss: 0.302727\tvalid_1's binary_logloss: 0.434527\n[70]\ttraining's binary_logloss: 0.285597\tvalid_1's binary_logloss: 0.434932\nEarly stopping, best iteration is:\n[66]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.433251\n","name":"stdout"},{"output_type":"stream","text":"[I 2020-10-02 12:33:23,538] Trial 27 finished with value: 0.4332512137886331 and parameters: {'max_bin': 499, 'num_leaves': 55}. Best is trial 0 with value: 0.4332512137886331.\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.505699\tvalid_1's binary_logloss: 0.532106\n[20]\ttraining's binary_logloss: 0.427825\tvalid_1's binary_logloss: 0.482279\n[30]\ttraining's binary_logloss: 0.377242\tvalid_1's binary_logloss: 0.456641\n[40]\ttraining's binary_logloss: 0.345424\tvalid_1's binary_logloss: 0.447083\n[50]\ttraining's binary_logloss: 0.323113\tvalid_1's binary_logloss: 0.440407\n[60]\ttraining's binary_logloss: 0.302727\tvalid_1's binary_logloss: 0.434527\n[70]\ttraining's binary_logloss: 0.285597\tvalid_1's binary_logloss: 0.434932\nEarly stopping, best iteration is:\n[66]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.433251\n","name":"stdout"},{"output_type":"stream","text":"[I 2020-10-02 12:33:23,703] Trial 28 finished with value: 0.4332512137886331 and parameters: {'max_bin': 418, 'num_leaves': 107}. Best is trial 0 with value: 0.4332512137886331.\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.505699\tvalid_1's binary_logloss: 0.532106\n[20]\ttraining's binary_logloss: 0.427825\tvalid_1's binary_logloss: 0.482279\n[30]\ttraining's binary_logloss: 0.377242\tvalid_1's binary_logloss: 0.456641\n[40]\ttraining's binary_logloss: 0.345424\tvalid_1's binary_logloss: 0.447083\n[50]\ttraining's binary_logloss: 0.323113\tvalid_1's binary_logloss: 0.440407\n[60]\ttraining's binary_logloss: 0.302727\tvalid_1's binary_logloss: 0.434527\n[70]\ttraining's binary_logloss: 0.285597\tvalid_1's binary_logloss: 0.434932\nEarly stopping, best iteration is:\n[66]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.433251\n","name":"stdout"},{"output_type":"stream","text":"[I 2020-10-02 12:33:24,252] Trial 29 finished with value: 0.4332512137886331 and parameters: {'max_bin': 369, 'num_leaves': 87}. Best is trial 0 with value: 0.4332512137886331.\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.505699\tvalid_1's binary_logloss: 0.532106\n[20]\ttraining's binary_logloss: 0.427825\tvalid_1's binary_logloss: 0.482279\n[30]\ttraining's binary_logloss: 0.377242\tvalid_1's binary_logloss: 0.456641\n[40]\ttraining's binary_logloss: 0.345424\tvalid_1's binary_logloss: 0.447083\n[50]\ttraining's binary_logloss: 0.323113\tvalid_1's binary_logloss: 0.440407\n[60]\ttraining's binary_logloss: 0.302727\tvalid_1's binary_logloss: 0.434527\n[70]\ttraining's binary_logloss: 0.285597\tvalid_1's binary_logloss: 0.434932\nEarly stopping, best iteration is:\n[66]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.433251\n","name":"stdout"},{"output_type":"stream","text":"[I 2020-10-02 12:33:24,408] Trial 30 finished with value: 0.4332512137886331 and parameters: {'max_bin': 283, 'num_leaves': 66}. Best is trial 0 with value: 0.4332512137886331.\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.505699\tvalid_1's binary_logloss: 0.532106\n[20]\ttraining's binary_logloss: 0.427825\tvalid_1's binary_logloss: 0.482279\n[30]\ttraining's binary_logloss: 0.377242\tvalid_1's binary_logloss: 0.456641\n[40]\ttraining's binary_logloss: 0.345424\tvalid_1's binary_logloss: 0.447083\n[50]\ttraining's binary_logloss: 0.323113\tvalid_1's binary_logloss: 0.440407\n[60]\ttraining's binary_logloss: 0.302727\tvalid_1's binary_logloss: 0.434527\n[70]\ttraining's binary_logloss: 0.285597\tvalid_1's binary_logloss: 0.434932\nEarly stopping, best iteration is:\n[66]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.433251\n","name":"stdout"},{"output_type":"stream","text":"[I 2020-10-02 12:33:24,559] Trial 31 finished with value: 0.4332512137886331 and parameters: {'max_bin': 383, 'num_leaves': 32}. Best is trial 0 with value: 0.4332512137886331.\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.505699\tvalid_1's binary_logloss: 0.532106\n[20]\ttraining's binary_logloss: 0.427825\tvalid_1's binary_logloss: 0.482279\n[30]\ttraining's binary_logloss: 0.377242\tvalid_1's binary_logloss: 0.456641\n[40]\ttraining's binary_logloss: 0.345424\tvalid_1's binary_logloss: 0.447083\n[50]\ttraining's binary_logloss: 0.323113\tvalid_1's binary_logloss: 0.440407\n[60]\ttraining's binary_logloss: 0.302727\tvalid_1's binary_logloss: 0.434527\n[70]\ttraining's binary_logloss: 0.285597\tvalid_1's binary_logloss: 0.434932\nEarly stopping, best iteration is:\n[66]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.433251\n","name":"stdout"},{"output_type":"stream","text":"[I 2020-10-02 12:33:24,731] Trial 32 finished with value: 0.4332512137886331 and parameters: {'max_bin': 419, 'num_leaves': 85}. Best is trial 0 with value: 0.4332512137886331.\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.505699\tvalid_1's binary_logloss: 0.532106\n[20]\ttraining's binary_logloss: 0.427825\tvalid_1's binary_logloss: 0.482279\n[30]\ttraining's binary_logloss: 0.377242\tvalid_1's binary_logloss: 0.456641\n[40]\ttraining's binary_logloss: 0.345424\tvalid_1's binary_logloss: 0.447083\n[50]\ttraining's binary_logloss: 0.323113\tvalid_1's binary_logloss: 0.440407\n[60]\ttraining's binary_logloss: 0.302727\tvalid_1's binary_logloss: 0.434527\n[70]\ttraining's binary_logloss: 0.285597\tvalid_1's binary_logloss: 0.434932\nEarly stopping, best iteration is:\n[66]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.433251\n","name":"stdout"},{"output_type":"stream","text":"[I 2020-10-02 12:33:24,886] Trial 33 finished with value: 0.4332512137886331 and parameters: {'max_bin': 388, 'num_leaves': 70}. Best is trial 0 with value: 0.4332512137886331.\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.505699\tvalid_1's binary_logloss: 0.532106\n[20]\ttraining's binary_logloss: 0.427825\tvalid_1's binary_logloss: 0.482279\n[30]\ttraining's binary_logloss: 0.377242\tvalid_1's binary_logloss: 0.456641\n[40]\ttraining's binary_logloss: 0.345424\tvalid_1's binary_logloss: 0.447083\n[50]\ttraining's binary_logloss: 0.323113\tvalid_1's binary_logloss: 0.440407\n[60]\ttraining's binary_logloss: 0.302727\tvalid_1's binary_logloss: 0.434527\n[70]\ttraining's binary_logloss: 0.285597\tvalid_1's binary_logloss: 0.434932\nEarly stopping, best iteration is:\n[66]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.433251\n","name":"stdout"},{"output_type":"stream","text":"[I 2020-10-02 12:33:25,037] Trial 34 finished with value: 0.4332512137886331 and parameters: {'max_bin': 487, 'num_leaves': 49}. Best is trial 0 with value: 0.4332512137886331.\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.505699\tvalid_1's binary_logloss: 0.532106\n[20]\ttraining's binary_logloss: 0.427825\tvalid_1's binary_logloss: 0.482279\n[30]\ttraining's binary_logloss: 0.377242\tvalid_1's binary_logloss: 0.456641\n[40]\ttraining's binary_logloss: 0.345424\tvalid_1's binary_logloss: 0.447083\n[50]\ttraining's binary_logloss: 0.323113\tvalid_1's binary_logloss: 0.440407\n[60]\ttraining's binary_logloss: 0.302727\tvalid_1's binary_logloss: 0.434527\n[70]\ttraining's binary_logloss: 0.285597\tvalid_1's binary_logloss: 0.434932\nEarly stopping, best iteration is:\n[66]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.433251\n","name":"stdout"},{"output_type":"stream","text":"[I 2020-10-02 12:33:25,188] Trial 35 finished with value: 0.4332512137886331 and parameters: {'max_bin': 334, 'num_leaves': 36}. Best is trial 0 with value: 0.4332512137886331.\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.505699\tvalid_1's binary_logloss: 0.532106\n[20]\ttraining's binary_logloss: 0.427825\tvalid_1's binary_logloss: 0.482279\n[30]\ttraining's binary_logloss: 0.377242\tvalid_1's binary_logloss: 0.456641\n[40]\ttraining's binary_logloss: 0.345424\tvalid_1's binary_logloss: 0.447083\n[50]\ttraining's binary_logloss: 0.323113\tvalid_1's binary_logloss: 0.440407\n[60]\ttraining's binary_logloss: 0.302727\tvalid_1's binary_logloss: 0.434527\n[70]\ttraining's binary_logloss: 0.285597\tvalid_1's binary_logloss: 0.434932\nEarly stopping, best iteration is:\n[66]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.433251\n","name":"stdout"},{"output_type":"stream","text":"[I 2020-10-02 12:33:25,341] Trial 36 finished with value: 0.4332512137886331 and parameters: {'max_bin': 360, 'num_leaves': 74}. Best is trial 0 with value: 0.4332512137886331.\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.505699\tvalid_1's binary_logloss: 0.532106\n[20]\ttraining's binary_logloss: 0.427825\tvalid_1's binary_logloss: 0.482279\n[30]\ttraining's binary_logloss: 0.377242\tvalid_1's binary_logloss: 0.456641\n[40]\ttraining's binary_logloss: 0.345424\tvalid_1's binary_logloss: 0.447083\n[50]\ttraining's binary_logloss: 0.323113\tvalid_1's binary_logloss: 0.440407\n[60]\ttraining's binary_logloss: 0.302727\tvalid_1's binary_logloss: 0.434527\n[70]\ttraining's binary_logloss: 0.285597\tvalid_1's binary_logloss: 0.434932\nEarly stopping, best iteration is:\n[66]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.433251\n","name":"stdout"},{"output_type":"stream","text":"[I 2020-10-02 12:33:25,493] Trial 37 finished with value: 0.4332512137886331 and parameters: {'max_bin': 441, 'num_leaves': 63}. Best is trial 0 with value: 0.4332512137886331.\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.505699\tvalid_1's binary_logloss: 0.532106\n[20]\ttraining's binary_logloss: 0.427825\tvalid_1's binary_logloss: 0.482279\n[30]\ttraining's binary_logloss: 0.377242\tvalid_1's binary_logloss: 0.456641\n[40]\ttraining's binary_logloss: 0.345424\tvalid_1's binary_logloss: 0.447083\n[50]\ttraining's binary_logloss: 0.323113\tvalid_1's binary_logloss: 0.440407\n[60]\ttraining's binary_logloss: 0.302727\tvalid_1's binary_logloss: 0.434527\n[70]\ttraining's binary_logloss: 0.285597\tvalid_1's binary_logloss: 0.434932\nEarly stopping, best iteration is:\n[66]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.433251\n","name":"stdout"},{"output_type":"stream","text":"[I 2020-10-02 12:33:25,666] Trial 38 finished with value: 0.4332512137886331 and parameters: {'max_bin': 375, 'num_leaves': 33}. Best is trial 0 with value: 0.4332512137886331.\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.505699\tvalid_1's binary_logloss: 0.532106\n[20]\ttraining's binary_logloss: 0.427825\tvalid_1's binary_logloss: 0.482279\n[30]\ttraining's binary_logloss: 0.377242\tvalid_1's binary_logloss: 0.456641\n[40]\ttraining's binary_logloss: 0.345424\tvalid_1's binary_logloss: 0.447083\n[50]\ttraining's binary_logloss: 0.323113\tvalid_1's binary_logloss: 0.440407\n[60]\ttraining's binary_logloss: 0.302727\tvalid_1's binary_logloss: 0.434527\n[70]\ttraining's binary_logloss: 0.285597\tvalid_1's binary_logloss: 0.434932\nEarly stopping, best iteration is:\n[66]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.433251\n","name":"stdout"},{"output_type":"stream","text":"[I 2020-10-02 12:33:25,815] Trial 39 finished with value: 0.4332512137886331 and parameters: {'max_bin': 320, 'num_leaves': 73}. Best is trial 0 with value: 0.4332512137886331.\n","name":"stderr"},{"output_type":"execute_result","execution_count":35,"data":{"text/plain":"{'max_bin': 427, 'num_leaves': 79}"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"実行結果の例\n```python\n{'max_bin': 427, 'num_leaves': 79}\n```\n\n指定した範囲内で試行回数だけ探索した結果得られた最良のハイパーパラメータが表示されています.こちらで改めて予測し直して,提出してみると,0.77033というスコアが出ました.偶然手動での調整と同じスコアになっています.探索範囲や試行回数を変えれば,より良いスコアが出るかもしれません."},{"metadata":{},"cell_type":"markdown","source":"# 6. submitのその前に！ 「Cross Validation」の大切さを知ろう\n3〜5章のNotebookでは,特徴量エンジニアリング・機械学習アルゴリズム・ハイパーパラメータの面で,スコアを上げていく方法を学びました.\nこのNotebookでは,機械学習モデルの性能を見積もる「validation」について解説します.\n### 提出時のスコアで検証してはいけないのか？\nこれまではモデルの性能について,主に実際にKaggleに提出した際のスコアで確認していました.しかし,この方法には次のような問題点があります.\n- 提出回数に制限がある.\n- public LBでいいスコアがでても,一部のデータに過学習した可能性がある.  \n  \nKaggleのコンペティションには1日の提出回数に制限があり,スコアが上がる保証もないのに気軽に提出するのは得策ではありません.1日の提出回数分しか試行錯誤ができない状況にも陥ってしまいます.\nまたメダルが獲得できるコンペティションでは,y_testの一部データのみがpublic LBに利用されておりスコアを随時確認できますが,最終順位は残りのprivate LBのデータに対する性能で決定します.  \n  \npublic LBで良いスコアが出ても,public LBのデータのみに過学習した結果の可能性があり,必ずしもprivate LBでの性能に寄与するかは分かりません.  \n  \npublic LBにどのようなデータが使われている分からないという問題もあります.極端な例ですが,Titanicのような二値分類の問題で0のラベルが付いたデータのみがpublic LBに使われている可能性を考えましょう.この場合,仮にpublic LBで高いスコアを出すモデルが作成できても,そのモデルは1のラベルを当てる性能がどれだけあるか確認できていません.  \n  \n以上の問題を踏まえて,Kaggleではvalidationスコアを通じて自分のモデルの性能を測るのが一般的です.ここでのvalidationは「モデルの汎化性能の検証」を意味します.汎化性能とは「未知のデータに対する性能」のことです.「過学習をしていないかの確認」というような意味合いでもあります.\n\n### ホールドアウト検証\n実は既に,LightGBMを利用する段階で「ホールドアウト検証」と呼ばれる一種のvalidationを実施していました.学習データセットを分割したうえでLightBGMを学習させていたことを思い出してください.\nこの検証用データセットは,自分で学習用データセットから切り出しているため,目的変数を含めて全容を把握できています.全体像の見えていないpublic LBのスコアに比べて,信頼性の高いスコアを得られる可能性を秘めています.  \n  \n検証用データセットに対する性能,すなわちvalidationスコアは,提出することなく手元で確認可能です.自分の気の済むだけ試行錯誤を回し,良いスコアを得た場合に実際にKaggleに提出するような運用が可能です.\n\n### 交差検証（Cross Validation）\nさて,ここで「Cross Validation」を実行すると,ホールドアウト検証の例よりも,更に汎用的に性能を確認できます.\n交差検証とは,複数回にわたって異なる方法でデータセットを分割し,それぞれでホールドアウト検証を実行する方法です.その平均を確認することで1回のホールドアウト検証で生じうる偏りに対する懸念を弱めることができます.(参考元元の図がわかりやすい) \n  \n``train_test_split``を複数回書いて実装することもできそうですが,より便利なパッケージが用意されています.``n_splits``は分割数で,ここではデータセットを5つに分けます."},{"metadata":{"trusted":true},"cell_type":"code","source":"# すでにlightGBM等実行している場合はX_train,Y_trainが分割されているのでX_train,y_trainをdatasetから切り離した状態\n# で実行してください.\nfrom sklearn.model_selection import KFold\n\n\ny_preds = []\nmodels = []\noof_train = np.zeros((len(X_train),))\ncv = KFold(n_splits=5, shuffle=True, random_state=0)\n\ncategorical_features = ['Embarked', 'Pclass', 'Sex']\n\nparams = {\n    'objective': 'binary',\n    'max_bin': 300,\n    'learning_rate': 0.05,\n    'num_leaves': 40\n}\n\nfor fold_id, (train_index, valid_index) in enumerate(cv.split(X_train)):\n    X_tr = X_train.loc[train_index, :]\n    X_val = X_train.loc[valid_index, :]\n    y_tr = y_train[train_index]\n    y_val = y_train[valid_index]\n\n    lgb_train = lgb.Dataset(X_tr, y_tr,\n                            categorical_feature=categorical_features)\n    lgb_eval = lgb.Dataset(X_val, y_val,\n                           reference=lgb_train,\n                           categorical_feature=categorical_features)\n\n    model = lgb.train(params, lgb_train,\n                      valid_sets=[lgb_train, lgb_eval],\n                      verbose_eval=10,\n                      num_boost_round=1000,\n                      early_stopping_rounds=10)\n\n    oof_train[valid_index] = model.predict(X_val, num_iteration=model.best_iteration)\n    y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n\n    y_preds.append(y_pred)\n    models.append(model)","execution_count":43,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1291: UserWarning: Using categorical_feature in Dataset.\n  warnings.warn('Using categorical_feature in Dataset.')\n","name":"stderr"},{"output_type":"stream","text":"Training until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.506339\tvalid_1's binary_logloss: 0.516198\n[20]\ttraining's binary_logloss: 0.428668\tvalid_1's binary_logloss: 0.446596\n[30]\ttraining's binary_logloss: 0.383412\tvalid_1's binary_logloss: 0.411961\n[40]\ttraining's binary_logloss: 0.35367\tvalid_1's binary_logloss: 0.397122\n[50]\ttraining's binary_logloss: 0.329451\tvalid_1's binary_logloss: 0.391041\n[60]\ttraining's binary_logloss: 0.307092\tvalid_1's binary_logloss: 0.38325\n[70]\ttraining's binary_logloss: 0.290771\tvalid_1's binary_logloss: 0.377067\n[80]\ttraining's binary_logloss: 0.274935\tvalid_1's binary_logloss: 0.373918\n[90]\ttraining's binary_logloss: 0.260335\tvalid_1's binary_logloss: 0.370602\nEarly stopping, best iteration is:\n[85]\ttraining's binary_logloss: 0.267203\tvalid_1's binary_logloss: 0.369116\nTraining until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.504211\tvalid_1's binary_logloss: 0.534594\n[20]\ttraining's binary_logloss: 0.422236\tvalid_1's binary_logloss: 0.481243\n[30]\ttraining's binary_logloss: 0.37173\tvalid_1's binary_logloss: 0.459512\n[40]\ttraining's binary_logloss: 0.341914\tvalid_1's binary_logloss: 0.453871\n[50]\ttraining's binary_logloss: 0.318699\tvalid_1's binary_logloss: 0.451806\n[60]\ttraining's binary_logloss: 0.296894\tvalid_1's binary_logloss: 0.449685\nEarly stopping, best iteration is:\n[58]\ttraining's binary_logloss: 0.301388\tvalid_1's binary_logloss: 0.449112\nTraining until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.506747\tvalid_1's binary_logloss: 0.507713\n[20]\ttraining's binary_logloss: 0.42895\tvalid_1's binary_logloss: 0.449278\n[30]\ttraining's binary_logloss: 0.384356\tvalid_1's binary_logloss: 0.41254\n[40]\ttraining's binary_logloss: 0.355218\tvalid_1's binary_logloss: 0.396228\n[50]\ttraining's binary_logloss: 0.333925\tvalid_1's binary_logloss: 0.387799\n[60]\ttraining's binary_logloss: 0.313799\tvalid_1's binary_logloss: 0.385787\n[70]\ttraining's binary_logloss: 0.298536\tvalid_1's binary_logloss: 0.38534\n[80]\ttraining's binary_logloss: 0.283249\tvalid_1's binary_logloss: 0.383338\n[90]\ttraining's binary_logloss: 0.270355\tvalid_1's binary_logloss: 0.385343\nEarly stopping, best iteration is:\n[80]\ttraining's binary_logloss: 0.283249\tvalid_1's binary_logloss: 0.383338\nTraining until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.497977\tvalid_1's binary_logloss: 0.525947\n[20]\ttraining's binary_logloss: 0.416873\tvalid_1's binary_logloss: 0.471473\n[30]\ttraining's binary_logloss: 0.368347\tvalid_1's binary_logloss: 0.446558\n[40]\ttraining's binary_logloss: 0.338844\tvalid_1's binary_logloss: 0.437886\n[50]\ttraining's binary_logloss: 0.318398\tvalid_1's binary_logloss: 0.437554\nEarly stopping, best iteration is:\n[41]\ttraining's binary_logloss: 0.336795\tvalid_1's binary_logloss: 0.437121\nTraining until validation scores don't improve for 10 rounds\n[10]\ttraining's binary_logloss: 0.494316\tvalid_1's binary_logloss: 0.552225\n[20]\ttraining's binary_logloss: 0.417327\tvalid_1's binary_logloss: 0.487758\n[30]\ttraining's binary_logloss: 0.370965\tvalid_1's binary_logloss: 0.453038\n[40]\ttraining's binary_logloss: 0.341695\tvalid_1's binary_logloss: 0.439283\n[50]\ttraining's binary_logloss: 0.319843\tvalid_1's binary_logloss: 0.4351\n[60]\ttraining's binary_logloss: 0.301123\tvalid_1's binary_logloss: 0.435584\nEarly stopping, best iteration is:\n[51]\ttraining's binary_logloss: 0.31821\tvalid_1's binary_logloss: 0.4347\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Cross Validationを実施した際は,全体の平均をvalidationスコアと見なすことが多いです.このスコアのことを「CVスコア」,省略して単に「CV」とも呼びます."},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = [\n    m.best_score['valid_1']['binary_logloss'] for m in models\n]\nscore = sum(scores) / len(scores)\nprint('===CV scores===')\nprint(scores)\nprint(score)","execution_count":44,"outputs":[{"output_type":"stream","text":"===CV scores===\n[0.3691161193267496, 0.44911229658021967, 0.3833384988458873, 0.43712149656630833, 0.43469994547894103]\n0.41467767135962114\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Cross Validationには,学習用データセットを無駄にしないという利点もあります,1回のみのホールドアウト検証では検証用データセットに該当する部分を学習に利用できていないですが,Cross Validationでは複数の分割を実施するので,全体としては与えられたデータセットをもれなく学習に利用できています.  \n  \nこの予測値を提出すると,0.76555というスコアが出ました.ホールドアウト検証の時よりも悪いスコアになってしまいました.その原因の一つは,データセットの分割方法だと推察されます.最後に,この部分を掘り下げて解説していきます."},{"metadata":{},"cell_type":"markdown","source":"### データセットの分割方法\nデータセットの分割に当たっては,データセットや課題設定の特徴を意識するのが大切です.  \n  \nここまで使っていたKFoldは,特にデータセットや課題設定の特徴を考慮することなくデータセットを分割します.例えば,学習用・検証用データセット内のy==1の割合を見てみると次のようになりました.fold: 2, 4などで顕著に割合が異なっていると分かります."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\n\ncv = KFold(n_splits=5, shuffle=True, random_state=0)\nfor fold_id, (train_index, valid_index) in enumerate(cv.split(X_train)):\n    X_tr = X_train.loc[train_index, :]\n    X_val = X_train.loc[valid_index, :]\n    y_tr = y_train[train_index]\n    y_val = y_train[valid_index]\n\n    print(f'fold: {fold_id}')\n    print(f'y_tr y==1 rate: {sum(y_tr)/len(y_tr)}')\n    print(f'y_val y==1 rate: {sum(y_val)/len(y_val)}')","execution_count":45,"outputs":[{"output_type":"stream","text":"fold: 0\ny_tr y==1 rate: 0.38342696629213485\ny_val y==1 rate: 0.3854748603351955\nfold: 1\ny_tr y==1 rate: 0.3856942496493689\ny_val y==1 rate: 0.37640449438202245\nfold: 2\ny_tr y==1 rate: 0.39831697054698456\ny_val y==1 rate: 0.3258426966292135\nfold: 3\ny_tr y==1 rate: 0.3856942496493689\ny_val y==1 rate: 0.37640449438202245\nfold: 4\ny_tr y==1 rate: 0.36605890603085556\ny_val y==1 rate: 0.4550561797752809\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"繰り返しになりますが,Kaggleの目的は未知のデータセットであるLBに対する性能を高めることです.未知のデータセットにおけるy==1の割合は誰にも正確には分からないですが,既存のデータセットである学習用データセットと同様の割合だと近似するのが一般的です.つまり,データセットはy==1の割合を保つように分割するのが理想的です.  \n  \ny==1の割合が均等でない場合,y==1を重要視したり逆に軽視したりと,機械学習アルゴリズムの学習がうまくいかない傾向にあります.このような状況では適切に特徴を学習できず,未知のデータセットに対する性能が劣化してしまう可能性があります.KFoldを用いた場合にスコアが悪化した原因もここにあると考えられます.  \n  \nちなみに以前にtrain_test_splitを利用した際には,stratifyという引数でy_trainを指定することで,割合を保ったままデータセットを2つに分割していました.  \n  \n割合を保ったままCross Validationを実施するためにはStratifiedKFoldを使います.学習用・検証用データセット内のy==1の割合が可能な範囲で均一に保たれています."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\nfor fold_id, (train_index, valid_index) in enumerate(cv.split(X_train, y_train)):\n    X_tr = X_train.loc[train_index, :]\n    X_val = X_train.loc[valid_index, :]\n    y_tr = y_train[train_index]\n    y_val = y_train[valid_index]\n\n    print(f'fold: {fold_id}')\n    print(f'y_tr y==1 rate: {sum(y_tr)/len(y_tr)}')\n    print(f'y_val y==1 rate: {sum(y_val)/len(y_val)}')","execution_count":46,"outputs":[{"output_type":"stream","text":"fold: 0\ny_tr y==1 rate: 0.38342696629213485\ny_val y==1 rate: 0.3854748603351955\nfold: 1\ny_tr y==1 rate: 0.38429172510518933\ny_val y==1 rate: 0.38202247191011235\nfold: 2\ny_tr y==1 rate: 0.38429172510518933\ny_val y==1 rate: 0.38202247191011235\nfold: 3\ny_tr y==1 rate: 0.38429172510518933\ny_val y==1 rate: 0.38202247191011235\nfold: 4\ny_tr y==1 rate: 0.38288920056100983\ny_val y==1 rate: 0.38764044943820225\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"この分割を用いて学習・予測を実行したところ,0.77511というスコアが出ました.KFoldの時よりも,ホールドアウト検証の時よりも良いスコアが出ています.\n分割の際に気を付けたいことは,目的変数の割合以外にも,以下のような点があります.\n- データセット内に時系列性がないか\n- データセット内にグループが存在しないか  \n\n詳細は「validationの切り方いろいろ（sklearnの関数まとめ）」Link : https://upura.hatenablog.com/entry/2018/12/04/224436"},{"metadata":{},"cell_type":"markdown","source":"# 7. 三人寄れば文殊の知恵！ アンサンブルを体験しよう\nアンサンブルとは,複数の機械学習モデルを組み合わせることで精度の高い予測値を獲得する手法です.  \n  \nアンサンブルはKaggleなどのコンペにおける最後の一押しとして,大きな成果を発揮する場合があります.近年は多くのチームで,取り組みの深さは違えど至極当然に用いられる手法になっている印象です.時には「実際の業務には役に立たない」と批判される対象にもなるものですが,個人的には「良くも悪くも機械学習コンペの特徴」と言えるものの一つだと思っています.\n\nアンサンブルについては「Kaggle Ensembling Guide」Link : https://mlwave.com/kaggle-ensembling-guide/ と呼ばれる有名な記事が存在します。ここでは「Kaggle Ensembling Guide」の冒頭からの具体例を引用して,アンサンブルの考え方を学びましょう.(アンサンブルの説明については参考記事がわかりやすい)\n\n### Titanicでの実験\nこのNotebookでは,これまで作ってきたNotebookの延長線上で実際にアンサンブルの効果を確認します.\n\nここでは「提出ファイルによるアンサンブル」を試してみましょう.これまで作成したランダムフォレストとLightGBMによる提出ファイルを利用します.\n\n- submission_lightgbm_skfold.csv\n- submission_lightgbm_holdout.csv\n- submission_randomforest.csv"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_lgbm_sk = pd.read_csv('../input/submit-files/submission_lightgbm_skfold.csv')\nsub_lgbm_ho = pd.read_csv('../input/submit-files/submission_lightgbm_holdout.csv')\nsub_rf = pd.read_csv('../input/submit-files/submission_randomforest.csv')","execution_count":47,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"「Kaggle Ensembling Guide」と同様に,多数決で予測値を決定します.3ファイルの予測値部分を合計し,合計が2以上の場合は全体としての予測値を1とします."},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.DataFrame(pd.read_csv('../input/titanic/test.csv')['PassengerId'])\nsub['Survived'] = sub_lgbm_sk['Survived'] + sub_lgbm_ho['Survived'] + sub_rf['Survived']\nsub['Survived'] = (sub['Survived'] >= 2).astype(int)\nsub.to_csv('submission_lightgbm_ensemble.csv', index=False)","execution_count":48,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"この予測値を提出すると,0.78468という過去最高のスコアが得られました.\n「Kaggle Ensembling Guide」には今回体験したような「提出ファイルによるアンサンブル」だけではなく,より高度な「Stacked Generalization (Stacking)」「Blending」といった様々な技法が紹介されています.より深くアンサンブルを勉強したい場合は,ご一読をオススメします."},{"metadata":{},"cell_type":"markdown","source":"### 提出(submit)\n最後にNotebook経由で提出するために予測値を提出ファイル形式に整えます.\n下のプログラムまでエラーなく実行できたら右上の``Save Version`` -> ``Save & Run All (Commit)``を選択 -> ``Save``をクリック"},{"metadata":{"trusted":true},"cell_type":"code","source":"# submit = pd.DataFrame(test['PassengerId'])\n# submit['Survived'] = list(map(int, y_pred))\n# submit.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}